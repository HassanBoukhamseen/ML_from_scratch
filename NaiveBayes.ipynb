{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NaiveBayes.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNXBTWtRHQ/tCTs8be0w99c"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"AI6Oa_hHPmOc"},"source":["# Naive Bayes"]},{"cell_type":"markdown","metadata":{"id":"hkgIQF6PmpQ5"},"source":["The Naive Bayes algorithm calculates the probability that a certain input vector X has a corrisponding label y. The algorithm basically computes the frequency of similar vectors in the training data and reports back the most common label for said vectors. \n","\n","$$P(y | X) = \\frac{P(X|y)P(y)}{P(X)}$$\n","\n","The algorithm is called naive because of the naive assumption that the features of the training data are independant. Nevertheless, the algorithm has proven useful in many cases. Consequently, this gives us:\n","\n","$$P(y | X) = \\frac{P(X=x_1|y) \\times P(X=x_2|y) ...P(X=x_n|y) \\times  P(y)}{P(X)}$$\n","\n","Since in computation, multiplying the probabilities, which are by definition small values between 0 and 1 might cause numerical issues, we can take the sum of the log probabilities instead. This gives us:\n","\n","$$P(y | X) = \\frac{\\sum log(P(X=x_1|y)) + log(P(X=x_2|y)) ... log(P(X=x_n|y)) + log(P(y))}{P(X)}$$\n","\n","Since we are looking for the most common label, y, $P(X)$ is of no significance to us, hence giving us the final expression:\n","\n","$$P(y | X=x) = argmax_y \\sum log(P(X=x_1|y)) + log(P(X=x_2|y)) ... log(P(X=x_n|y)) + log(P(y))$$\n","\n","In order to calculate the probability of x given y, we use a normal distribution. However, depnding on the dataset, one could choose to implement the algorithm using a bernouli or multinomial distribution.\n","\n","$$ f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}}e^{-\\frac{1}{2}(\\frac{x-\\mu}{\\sigma})^2}$$\n","\n","Below is a step by step implementation of the algorithm"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"In9u-Glrqz_i","executionInfo":{"status":"ok","timestamp":1632525124772,"user_tz":300,"elapsed":141,"user":{"displayName":"Hassan Boukhamseen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13575020213521366718"}},"outputId":"2130b220-1de3-4d2f-a240-3c8970124faf"},"source":["from sklearn.model_selection import train_test_split\n","from sklearn import datasets\n","import numpy as np\n","\n","\n","X, y = datasets.make_classification(n_samples=10000, n_features=10, n_classes=2, random_state=123)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n","print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(8000, 10) (2000, 10) (8000,) (2000,)\n"]}]},{"cell_type":"markdown","metadata":{"id":"zPBz8R1yPvEj"},"source":["## Defining the parameters of the distribtuion and the prior"]},{"cell_type":"markdown","metadata":{"id":"l_MYmJ7LvUp6"},"source":["We will define three Matrices that will make our life easier during computation: \n","$$\\mu_x = \\begin{bmatrix}\n","\\mathbb{E}[{x^{(1)}}|y_1] & \\mathbb{E}[{x^{(1)}}|y_2]\\\\\n","\\mathbb{E}[{x^{(2)}}|y_1] & \\mathbb{E}[{x^{(2)}}|y_2]\\\\\n","\\mathbb{E}[{x^{(3)}}|y_1] & \\mathbb{E}[{x^{(3)}}|y_2]\\\\\n","\\cdots & \\cdots \\\\\n","\\mathbb{E}[{x^{(j)}}|y_1] & \\mathbb{E}[{x^{(j)}}|y_2]\\\\\n","\\end{bmatrix}$$\n","\\\n","$$\\sigma_x = \\begin{bmatrix}\n","\\sigma({x^{(1)}}|y_1) & \\sigma({x^{(1)}}|y_2)\\\\\n","\\sigma({x^{(2)}}|y_1) & \\sigma({x^{(2)}}|y_2)\\\\\n","\\sigma({x^{(3)}}|y_1) & \\sigma({x^{(3)}}|y_2)\\\\\n","\\cdots & \\cdots \\\\\n","\\sigma({x^{(j)}}|y_1) & \\sigma({x^{(j)}}|y_2)\\\\\n","\\end{bmatrix}$$\n","\\\n","$$P(y) = \\begin{bmatrix}\n","log(P(y_1))&Plog((y_2))\\\\\n","\\end{bmatrix}$$\n","\n","where j is the number of features "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b0E6GkE4sDqW","executionInfo":{"status":"ok","timestamp":1632525124912,"user_tz":300,"elapsed":8,"user":{"displayName":"Hassan Boukhamseen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13575020213521366718"}},"outputId":"265d780a-b811-4e37-c498-95f82617ffa0"},"source":["#lets define the mean, std, and the log prior\n","#we need to calculate the mean for each feature in x\n","n_samples, n_features = X_train.shape\n","classes = np.unique(y_train)\n","\n","mean = np.zeros((n_features, classes.shape[0]), dtype=np.float64)\n","std = np.zeros((n_features, classes.shape[0]), dtype=np.float64)\n","prior = np.zeros(classes.shape[0], dtype=np.float64)\n","\n","for c in classes:\n","    curr_data = X[y == c]\n","    mean[:, c] = curr_data.mean(axis=0)\n","    std[:, c] = curr_data.std(axis=0)\n","    prior[c] = np.log(y[y==c].shape[0]/y.shape[0])\n","\n","\n","print(mean.shape, std.shape, prior.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(10, 2) (10, 2) (2,)\n"]}]},{"cell_type":"markdown","metadata":{"id":"cHsbE6wiP6Nf"},"source":["## Defining the posterior"]},{"cell_type":"markdown","metadata":{"id":"aiWbDf4vFMkn"},"source":["We will now fit the distribution to the mean and standard deviation of the train dataset and use it to get probabilities of features from the test set. This is basically getting $P(y|X)$. We should have the resulting matrix:\n","\\\n","$$P(y|X)= \\begin{bmatrix} \\bigg[\\log p(y=0) + \\sum_{j=0}^{10} \\log p(x_1^{(j)}|y=0) \\bigg] & \\bigg[\\log p(y=1) + \\sum_{j=0}^{10} \\log p(x_1^{(j)}|y=1) \\bigg] \\\\\n","\\bigg[\\log p(y=0) + \\sum_{j=0}^{10} \\log p(x_2^{(j)}|y=0) \\bigg] & \\bigg[\\log p(y=1) + \\sum_{j=0}^{10} \\log p(x_2^{(j)}|y=1) \\bigg] \\\\\n","\\cdots & \\cdots \\\\\n","\\bigg[\\log p(y=0) + \\sum_{j=0}^{10} \\log p(x_N^{(j)}|y=0) \\bigg] & \\bigg[\\log p(y=1) + \\sum_{j=0}^{10} \\log p(x_N^{(j)}|y=1) \\bigg] \\\\\n","\\end{bmatrix}$$ \n","\n","where N is the number of datapoints and j is the number of features\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"mmiQMu3bykz1"},"source":["N, d = X_test.shape\n","posterior = np.zeros((classes.shape[0], N))\n","# your code here\n","for i, c in enumerate(classes):\n","    posterior_i = np.array([np.sum(np.log(1/(std[:, c]*np.sqrt(2*np.pi)))-(1/2)*((x-mean[:, c])**2)/std[:, c]**2).item() for x in X_test])\n","    posterior[c, :] = prior[c] + posterior_i\n","preds = posterior.T.argmax(axis=1)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pR4BUzRrE-iD"},"source":["def accuracy(preds, y_test):\n","    return np.mean(preds == y_test)*100"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vB9Yes60E-tP","executionInfo":{"status":"ok","timestamp":1632525133381,"user_tz":300,"elapsed":126,"user":{"displayName":"Hassan Boukhamseen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13575020213521366718"}},"outputId":"15477692-bfe8-43cf-abec-899737567018"},"source":["print(\"Naive Bayes classification accuracy\", accuracy(y_test, preds))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Naive Bayes classification accuracy 92.05\n"]}]},{"cell_type":"markdown","metadata":{"id":"MJFN23mtJYeU"},"source":["## Putting Everything Togethter\n"]},{"cell_type":"code","metadata":{"id":"FYfwQoOfJdeG"},"source":["class NaiveBayes:\n","    def fit(self, X_train, y_train):\n","        n_samples, n_features = X_train.shape\n","        classes = np.unique(y_train)\n","\n","        mean = np.zeros((n_features, classes.shape[0]), dtype=np.float64)\n","        std = np.zeros((n_features, classes.shape[0]), dtype=np.float64)\n","        prior = np.zeros(classes.shape[0], dtype=np.float64)\n","\n","        for c in classes:\n","            curr_data = X[y == c]\n","            mean[:, c] = curr_data.mean(axis=0)\n","            std[:, c] = curr_data.std(axis=0)\n","            prior[c] = np.log(y[y==c].shape[0]/y.shape[0])\n","\n","    def predict(self, X_test):\n","        N, d = X_test.shape\n","        posterior = np.zeros((classes.shape[0], N))\n","        for i, c in enumerate(classes):\n","            posterior_i = np.array([np.sum(np.log(1/(std[:, c]*np.sqrt(2*np.pi)))-(1/2)*\n","                                           ((x-mean[:, c])**2)/std[:, c]**2).item() for x in X_test])\n","            posterior[c, :] = prior[c] + posterior_i\n","        preds = posterior.T.argmax(axis=1)\n","        return preds"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ocS3NAEXJwo3","executionInfo":{"status":"ok","timestamp":1632527091326,"user_tz":300,"elapsed":123,"user":{"displayName":"Hassan Boukhamseen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13575020213521366718"}},"outputId":"10fdb6a0-cd22-4273-8cb5-c8c78930cc9c"},"source":["NBClassifier = NaiveBayes()\n","NBClassifier.fit(X_train, y_train)\n","preds = NBClassifier.predict(X_test)\n","acc = accuracy(y_test, preds)\n","print(f\"Naive Bayes classification accuracy {acc}\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Naive Bayes classification accuracy 92.05\n"]}]}]}