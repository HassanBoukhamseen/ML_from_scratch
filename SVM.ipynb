{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SVM.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPp3frGA68WGiiCsINWsQHV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"4RvCDgAeKnK0"},"source":["Linear classifiers are a family of classifiers that have a linear decision boundry that seperates the classes. For instance, a linear binary classifier would classify points that are to the right of the decision boundary as 1, and the ones that are to the left of the decision boundary as -1.\n","\n","## The Support Vector Machine (SVM)\n","\n","Support vector machines are linear classifiers that use the following decision boundary:\n","\\\n","$$W^TX - \\beta = 0$$\n","\\\n","where points that are to the right of this hyper-plane are given by:\n","\\\n","$$W^TX - \\beta = 1$$\n","\\\n","and points that are to the left are given by:\n","$$W^TX - \\beta \\le -1$$\n","\\\n","Take a cluster points, and suppose that the task is to classify them. The SVM draws a hyper-plane to seperate the cluster into blobs. The way it accomplished this is by considering the point of each class $a$ that is closest to the other class $b$ and vice versa as support vectors. The task here to draw a hyper-plane that passes through these two vectors such that it maximizes the distance between them. Lets see how that is formulated:\n","$$W^TX + \\beta = 1$$\n","\n","when $y = 1$, we have:\\\n","$$W^TX - \\beta \\ge 1$$\n","when $y = -1$, we have:\\\n","$$W^TX - \\beta \\le -1$$\n","Thus:\\\n","$$y(W^TX - \\beta) = 1$$\\\n","To choose an appropriate loss function, we need to consider a couple of key points:\n","\n","\n","1.   When a datapoint is classified correctly, and is far from the decision boundary, then it should not be penalized\n","2.   when a datapoint is misclassified, the penalty should correlate with how close it is to the decision boundary\n","3.   if a point was classified correctly, but was close to the decision boundary, it should be penalized, sense close datapoints in the test data might be missclassified\n","\n","Taking these points into consideration, one loss function that fits is the hinge loss, given by:\n","\n","$$C(y, \\hat{y}) = \\frac{1}{N}\\sum_{i=0}^{N}{max(0, 1-y\\hat{y})}$$\n","\n","We add a regularization term #############\n","\n","$$C(y, \\hat{y}) = \\frac{1}{N}\\sum_{i=0}^{N}{max(0, 1 - y\\hat{y}) + \\lambda \\frac{1}{2}||w||}$$\n","\n","In this case, the gradient become:\n","\n","$$ \\begin{bmatrix}\n","\\frac{dC}{dW} \\\\\n","\\frac{dC}{d\\beta} \\\\\n","\\end{bmatrix} = \n","\\begin{bmatrix}\n","    \\begin{cases} \n","      \\lambda w & y\\hat{y}^{(i)} \\ge 1 \\\\\n","      \\lambda w - y^{(i)}x & y\\hat{y}^{(i)} < 1 \n","    \\end{cases}\n","\\\\\n","    \\begin{cases} \n","      0 &  y\\hat{y}^{(i)} \\ge 1 \\\\ \\\\\n","      y^{(i)} & y\\hat{y}^{(i)} < 1\n","    \\end{cases}\n","\\\\\n","\\end{bmatrix}$$\n","The update rule is simply:\n","$$\\hat{W} = W - \\alpha \\frac{dC}{dW}$$\n","$$\\hat{\\beta} = W - \\alpha \\frac{dC}{d\\beta}$$\n","\n","where $\\alpha$ is the learning rate\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"5tMqeKmOKjZe"},"source":["import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn import datasets\n","import numpy as np\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-DgPbvVjTDDr"},"source":["X, y = datasets.make_blobs(n_samples=200, n_features=2, centers=2, cluster_std=1.05, random_state=111)\n","y = np.where(y <=0, -1, 1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LO0lh-gRYTlo"},"source":["def accuracy(preds, y):\n","\n","    return np.mean(preds == y)*100"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wI_vXTsCTomJ","executionInfo":{"status":"ok","timestamp":1632637674577,"user_tz":300,"elapsed":2146,"user":{"displayName":"Hassan Boukhamseen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13575020213521366718"}},"outputId":"e384b456-5dd4-48cd-a66a-dc887c52a72d"},"source":["epochs = 1000\n","learning_rate = 0.001\n","reg_constant = 0.01\n","n_samples, n_features = X.shape\n","w = np.random.randn(n_features)\n","b = np.random.randn(1).item()\n","\n","for e in range(epochs):\n","    for i, x_i in enumerate(X):\n","        if y[i] * (np.dot(x_i, w) - b) >= 1:\n","            w -= learning_rate * reg_constant * w\n","        else:\n","            w -= learning_rate * (reg_constant * w - x_i*y[i])\n","            b -= learning_rate * y[i]\n","\n","preds = np.sign(np.dot(X, w) - b)\n","print(f'The accuracy is {accuracy(preds, y)} where w = {w} and b = {b}')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The accuracy is 100.0 where w = [-0.15311815  0.61685734] and b = 0.712406849811307\n"]}]}]}